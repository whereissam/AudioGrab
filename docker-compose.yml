version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: audiograb-api
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./output:/app/output
      - ./data:/app/data
    environment:
      - DOWNLOAD_DIR=/app/output
      - HOST=0.0.0.0
      - PORT=8000
      - TWITTER_AUTH_TOKEN=${TWITTER_AUTH_TOKEN:-}
      - TWITTER_CT0=${TWITTER_CT0:-}
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}
      - WHISPER_SERVICE_URL=http://whisper:8001
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000,http://localhost:5173}
      - API_KEY=${API_KEY:-}
      - SENTRY_DSN=${SENTRY_DSN:-}
      - STORAGE_CLEANUP_ENABLED=${STORAGE_CLEANUP_ENABLED:-true}
      - MAX_STORAGE_GB=${MAX_STORAGE_GB:-}
      - MIN_FREE_SPACE_GB=${MIN_FREE_SPACE_GB:-}
    depends_on:
      whisper:
        condition: service_healthy
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  whisper:
    build:
      context: ./faster-whisper/docker
      dockerfile: Dockerfile.service
    container_name: audiograb-whisper
    ports:
      - "${WHISPER_PORT:-8001}:8001"
    volumes:
      - whisper-models:/root/.cache/huggingface
      - ./output:/app/output:ro
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cuda}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-float16}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: audiograb-frontend
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
    depends_on:
      - api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: audiograb-ollama
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    # Uncomment below for GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    restart: unless-stopped

volumes:
  whisper-models:
    name: audiograb-whisper-models
  ollama-models:
    name: audiograb-ollama-models
